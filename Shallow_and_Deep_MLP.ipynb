{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1eBZglauqvaZJR0O35oIXk-syHKKQllna",
      "authorship_tag": "ABX9TyMorgzvHzTZfZ1clgEBdrnc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qortmdgh4141/Comparing-Performance-of-Shallow-and-Deep-MLP-for-Regression-Analysis/blob/main/Shallow_and_Deep_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 패키지 설정**"
      ],
      "metadata": {
        "id": "g4PrYBjjfO7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5LhFAUm3mxKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP의 회귀분석을 위해서 sklearn의 MLPRegressor 모듈을 사용\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ul8pOMMY-rqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. 데이터 준비**"
      ],
      "metadata": {
        "id": "DfJ8BCbYfvfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력값 : X 변수값의 범위는 1~100 사이의 정수로 지정\n",
        "# 출력값 : X의 값에 대응하는 Y값 계산\n",
        "x = np.array(range(1, 101))\n",
        "y = np.sqrt(x)\n",
        "\n",
        "print(f\"x 값의 범위 : {np.min(x)} ~ {np.max(x)}\")\n",
        "print(f\"y 값의 범위 : {int(np.min(y))} ~ {int(np.max(y))}\")"
      ],
      "metadata": {
        "id": "tvMd0D0SfIJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. 탐색적 데이터 분석**"
      ],
      "metadata": {
        "id": "oITHFK4RhmwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X와 Y간의 관계를 그래프로 그려보면, X 값이 증가할 때 Y 값도 점차 증가하지만, \n",
        "# 그 증가 속도는 X 값이 증가함에 따라 감소하는 비선형 곡선\n",
        "# 이러한 형태를 지니는 완만하게 증가하는 곡선을 로그 곡선(log curve)이라고 함\n",
        "plt.figure(figsize=(12, 3))\n",
        "plt.scatter(x, y, color='g')\n",
        "plt.title('y=sqrt(x)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend(['Actual Data']) \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QBWeOEFKhos_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. 데이터 분리**"
      ],
      "metadata": {
        "id": "HDuDCFAClikQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습용과 테스트용 데이터를 7:3으로 분리\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1234)\n",
        "    \n",
        "print(f\"1) 학습용 입력 데이터(X) 형상 : {x_train.shape}\")\n",
        "print(f\"2) 학습용 정답 데이터(Y) 형상 : {y_train.shape}\")\n",
        "print(f\"3) 평가용 입력 데이터(X) 형상 : {x_test.shape}\")\n",
        "print(f\"4) 평가용 정답 데이터(Y) 형상 : {x_test.shape}\")   \n",
        "\n"
      ],
      "metadata": {
        "id": "izvBl0l_kTSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. 피처 스케일링**"
      ],
      "metadata": {
        "id": "8ceb66rNuKDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 세트의 차원을 1열 구조로 변형\n",
        "x_train = x_train.reshape(-1, 1)\n",
        "x_test = x_test.reshape(-1, 1)\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "print(f\"1) 학습용 입력 데이터(X) 형상 : {x_train.shape}\")\n",
        "print(f\"2) 학습용 정답 데이터(Y) 형상 : {y_train.shape}\")\n",
        "print(f\"3) 평가용 입력 데이터(X) 형상 : {x_test.shape}\")\n",
        "print(f\"4) 평가용 정답 데이터(Y) 형상 : {x_test.shape}\")  "
      ],
      "metadata": {
        "id": "8pZ-N409mrEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최솟값은 0, 최댓값은 1이 되도록 학습 데이터에 대해 정규화\n",
        "# 피처 스케일링 : 학습 데이터의 입력 값\n",
        "scalerX = MinMaxScaler()\n",
        "scalerX.fit(x_train)\n",
        "x_train_norm = scalerX.transform(x_train)\n",
        "\n",
        "# 피처 스케일링 : 학습 데이터의 출력 값\n",
        "scalerY = MinMaxScaler()\n",
        "scalerY.fit(y_train)\n",
        "y_train_norm = scalerY.transform(y_train)\n",
        "\n",
        "# 피처 스케일링 : 테스트 데이터의 입출력 값\n",
        "x_test_norm = scalerX.transform(x_test)\n",
        "y_test_norm = scalerY.transform(y_test)"
      ],
      "metadata": {
        "id": "duDnjfGcuD4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. 얕은 모델(Shallow Model) 모형화 및 학습**"
      ],
      "metadata": {
        "id": "S5FzERhrveb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. 입출력 노드 : 1개\n",
        "   - 학습 시에 입출력 값이 각각 1개이기 때문에, 그에 대응하는 입출력 노드도 각각 1개씩 존재\n",
        "\n",
        "2. 은닉층 개수 / 노드 : 1개 / 2개\n",
        "    - 총 1개의 은닉층이 존재하며 각 은닉층에는 2개의 노드가 존재\n",
        "\n",
        "3. 활성화 함수 :  로지스틱 함수(시그모이드 함수)\n",
        "   - 0과 1 사이의 값을 출력하는 S자 형태의 비선형 함수인 로지스틱 함수(시그모이드 함수)로 설정\n",
        "\n",
        "4. 최적화 알고리즘 \n",
        "   - 작은 데이터셋에서는 일반적으로 수렴 속도가 빠르고, \n",
        "     대부분의 경우 다른 최적화 알고리즘보다 더 나은 성능을 보이는 L-BFGS (Limited-memory BFGS)를 사용\n",
        "\n",
        "5. 비용함수 : \n",
        "   - 예측값과 실제값의 차이를 제곱한 값의 평균을 계산함으로써, \n",
        "     예측값과 실제값 사이의 오차를 잘 나타내는 MSE를 사용\n",
        "\n",
        "6. 최대 학습 반복 횟수 : 100회\n",
        "\"\"\"\n",
        "\n",
        "# 모형화\n",
        "shallow_model = MLPRegressor(hidden_layer_sizes=(2,), activation='logistic'\n",
        "                                , solver='lbfgs', max_iter=100)\n",
        "\n",
        "# 학습\n",
        "shallow_model.fit(x_train_norm, y_train_norm)"
      ],
      "metadata": {
        "id": "X8GSqi2KvNGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. 얕은 모델(Shallow Model) 예측**"
      ],
      "metadata": {
        "id": "F2SMwzZc301U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측\n",
        "y_pred = shallow_model.predict(x_test_norm)\n",
        "# 예측 값의 데이터를 1열 구조로 변형\n",
        "y_pred = y_pred.reshape(-1,1)\n",
        "# 예측 값을 실제 스케일로 역변환\n",
        "y_pred_inverse = scalerY.inverse_transform(y_pred)\n",
        "print(f\"Shallow Model - 예측 값의 범위 : {np.min(y_pred_inverse)} ~ {np.max(y_pred_inverse)}\")"
      ],
      "metadata": {
        "id": "waQJDxWu0_QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 값 대비 절대 오차의 평균 백분율(MAPE)로 정확도를 계산\n",
        "# MAPE(오차 측정) 값이 0에 가까울수록 모델의 예측 성능은 일반적으로 좋다고 판단\n",
        "shallow_model_mape = mean_absolute_percentage_error(y_test, y_pred_inverse)\n",
        "print(f\"Shallow Model - MAPE : {shallow_model_mape:.2f}\")"
      ],
      "metadata": {
        "id": "kKkc6YKZ-AnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1~100 사이의 X값에 대응하는 Y의 실제 값과 테스트 데이터로 예측한 값을 산포도로 출력하면, \n",
        "# 예측 값이 실제 값과 비슷한 결과를 도출 \n",
        "# 실제 데이터의 분포\n",
        "plt.figure(figsize=(12, 3))\n",
        "plt.scatter(x, y, color='g')\n",
        "# 테스트 데이터의 분포\n",
        "plt.scatter(x_test, y_pred_inverse, color='r')\n",
        "\n",
        "plt.title('y=sqrt(x)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend(['Actual Data', 'Predicted Data by Shallow Model']) \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sAeFaYO71rzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. 깊은 모델(Deep Model) 모형화 및 학습**"
      ],
      "metadata": {
        "id": "ON4F7_PCBA3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. 입출력 노드 : 1개\n",
        "   - 학습 시에 입출력 값이 각각 1개이기 때문에, 그에 대응하는 입출력 노드도 각각 1개씩 존재\n",
        "\n",
        "2. 은닉층 개수 / 노드 : 4개 / 8개\n",
        "    - 총 4개의 은닉층이 존재하며 각 은닉층에는 8개의 노드가 존재\n",
        "\n",
        "3. 활성화 함수 :  로지스틱 함수(시그모이드 함수)\n",
        "   - 0과 1 사이의 값을 출력하는 S자 형태의 비선형 함수인 로지스틱 함수(시그모이드 함수)로 설정\n",
        "\n",
        "4. 최적화 알고리즘 \n",
        "   - 작은 데이터셋에서는 일반적으로 수렴 속도가 빠르고, \n",
        "     대부분의 경우 다른 최적화 알고리즘보다 더 나은 성능을 보이는 L-BFGS (Limited-memory BFGS)를 사용\n",
        "\n",
        "5. 비용함수 : \n",
        "   - 예측값과 실제값의 차이를 제곱한 값의 평균을 계산함으로써, \n",
        "     예측값과 실제값 사이의 오차를 잘 나타내는 MSE를 사용\n",
        "\n",
        "6. 최대 학습 반복 횟수 : 100회\n",
        "\"\"\"\n",
        "\n",
        "# 모형화\n",
        "deep_model = MLPRegressor(hidden_layer_sizes=(8,8,8,8), activation='logistic'\n",
        "                                , solver='lbfgs', max_iter=100)\n",
        "\n",
        "# 학습\n",
        "deep_model.fit(x_train_norm, y_train_norm)"
      ],
      "metadata": {
        "id": "ptPi3oPABDah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. 깊은 모델(Deep Model) 예측**"
      ],
      "metadata": {
        "id": "JtTk9nMJmS8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측\n",
        "y_pred_extended = deep_model.predict(x_test_norm)\n",
        "# 예측 값의 데이터를 1열 구조로 변형\n",
        "y_pred_extended = y_pred_extended.reshape(-1,1)\n",
        "# 예측 값을 실제 스케일로 역변환\n",
        "y_pred_inverse_extended = scalerY.inverse_transform(y_pred_extended)\n",
        "print(f\"예측 값의 범위 : {np.min(y_pred_inverse_extended)} ~ {np.max(y_pred_inverse_extended)}\")"
      ],
      "metadata": {
        "id": "3cqkMyWLBpM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 값 대비 절대 오차의 평균 백분율(MAPE)로 정확도를 계산\n",
        "# MAPE(오차 측정) 값이 0에 가까울수록 모델의 예측 성능은 일반적으로 좋다고 판단\n",
        "deep_model_mape = mean_absolute_percentage_error(y_test, y_pred_inverse_extended)\n",
        "print(f\"Deep Model - MAPE : {deep_model_mape:.2f}\")"
      ],
      "metadata": {
        "id": "5faFK0grBvXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1~100 사이의 X값에 대응하는 Y의 실제 값과 테스트 데이터로 예측한 값을 산포도로 출력 \n",
        "# 실제 데이터의 분포\n",
        "plt.figure(figsize=(12, 3))\n",
        "plt.scatter(x, y, color='g')\n",
        "# 테스트 데이터의 분포\n",
        "plt.scatter(x_test, y_pred_inverse_extended, color='b')\n",
        "\n",
        "plt.title('y=sqrt(x)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend(['Actual Data', 'Predicted Data by Deep Model']) \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jMoyRzqZBtTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1~100 사이의 X값에 대응하는 Y의 실제 값과 테스트 데이터로 예측한 값을 산포도로 출력하면, 예측 값이 실제 값과 비슷한 결과를 도출 \n",
        "# 실제 데이터의 분포\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.scatter(x, y, color='g')\n",
        "# 테스트 데이터의 분포\n",
        "plt.scatter(x_test, y_pred_inverse, color='r')\n",
        "plt.scatter(x_test, y_pred_inverse_extended, color='b')\n",
        "\n",
        "plt.title('y=sqrt(x)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend(['Actual Data', 'Predicted Data by Shallow Model', 'Predicted Data by Deep Model'],fontsize=8)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3haTK--hJDJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradientbars(bars, cmap_list):\n",
        "    # cmap 가중치 설정\n",
        "    grad = np.atleast_2d(np.linspace(0,1,256)).T\n",
        "    # 플롯 영역 재설정\n",
        "    ax = bars[0].axes\n",
        "    lim = ax.get_xlim()+ax.get_ylim()\n",
        "    ax.axis(lim)\n",
        "    # 각 막대에 색 입히기\n",
        "    max = 0\n",
        "    for i, bar in enumerate(bars):\n",
        "        bar.set_facecolor(\"none\")\n",
        "        x,y = bar.get_xy()\n",
        "        w, h = bar.get_width(), bar.get_height()\n",
        "        ax.imshow(grad, extent=[x,x+w,y,y+h], aspect=\"auto\", cmap=cmap_list[i])\n",
        "        plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, f' ==> {df.Mape[i]:.2f}', ha='left', va='center', fontsize=10, color='black')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,4))\n",
        "df = pd.DataFrame({'Model':['Shallow Model', 'Deep Model'], 'Mape':[shallow_model_mape, deep_model_mape]})\n",
        "cmap_color = ['viridis_r', 'YlOrRd']\n",
        "gradientbars(ax.barh(df.Model, df.Mape), cmap_color)\n",
        "\n",
        "plt.title(f\"Comparison of MAPE values between shallow and deep models\", fontsize=12)\n",
        "plt.xlabel('Mape', fontsize=10)\n",
        "plt.xlim([0, 0.4])\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yet-LIVGqpeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(12,2))\n",
        "df = pd.DataFrame({'Model':['Shallow', 'Deep'], 'Mape':[shallow_model_mape, deep_model_mape]})\n",
        "cmap_color = ['viridis_r', 'YlOrRd']\n",
        "bars = ax.barh(df.Model, df.Accuracy, color=cmap_color)\n",
        "\n",
        "plt.title(\"Comparison of MAPE values between shallow and deep models\", fontsize=10)\n",
        "plt.xlabel('Mape', fontsize=8)\n",
        "plt.xlim([0, 0.4])\n",
        "plt.xticks(fontsize=8)\n",
        "plt.yticks(fontsize=8)\n",
        "\n",
        "for i, bar in enumerate(bars):\n",
        "    ax.text(bar.get_width() + 0.01, i, str(round(bar.get_width(), 3)), fontsize=8, va='center')\n",
        "    \n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mrRixaM0t4wC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
